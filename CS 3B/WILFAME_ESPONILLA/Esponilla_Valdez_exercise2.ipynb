{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8306272f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem B: Breast Cancer Dataset\n",
      "\n",
      "Hidden Layer 1 Output (ReLU):\n",
      "  Neuron 1: 1.3360\n",
      "  Neuron 2: 10.3830\n",
      "  Neuron 3: 9.0095\n",
      "\n",
      "Hidden Layer 2 Output (Sigmoid):\n",
      "  Neuron 1: 0.9190\n",
      "  Neuron 2: 1.0000\n",
      "\n",
      "Final Output (Sigmoid):\n",
      "  Predicted probability (Malignant = 1): 0.5850\n",
      "\n",
      "Predicted Class: Malignant (1)\n",
      "\n",
      "Loss:\n",
      "  MSE: 0.1722\n",
      "  Cross-Entropy: 0.5362\n"
     ]
    }
   ],
   "source": [
    "# Problem B: Breast Cancer Dataset\n",
    "import math\n",
    "from neural_network_helper import weighted_sum, activation_function, calculate_loss\n",
    "\n",
    "# Breast Cancer Dataset inputs (Mean Radius, Mean Texture, Mean Smoothness)\n",
    "X = [14.1, 20.3, 0.095]\n",
    "target_output = [1]  # Malignant\n",
    "\n",
    "# First Hidden Layer (ReLU)\n",
    "W1 = [\n",
    "    [0.5, -0.3, 0.8],\n",
    "    [0.2, 0.4, -0.6],\n",
    "    [-0.7, 0.9, 0.1]\n",
    "]\n",
    "B1 = [0.3, -0.5, 0.6]\n",
    "\n",
    "# Second Hidden Layer (Sigmoid)\n",
    "W2 = [\n",
    "    [0.6, -0.2, 0.4],\n",
    "    [-0.3, 0.5, 0.7]\n",
    "]\n",
    "B2 = [0.1, -0.8]\n",
    "\n",
    "# Output Layer (Sigmoid, single node)\n",
    "W3 = [[0.7, -0.5]]\n",
    "B3 = [0.2]\n",
    "\n",
    "# Step 1: First Hidden Layer\n",
    "H1 = []\n",
    "for i in range(len(W1)):\n",
    "    z = weighted_sum(X, W1[i], B1[i])\n",
    "    a = activation_function(z, \"relu\")\n",
    "    H1.append(a)\n",
    "\n",
    "print(\"Problem B: Breast Cancer Dataset\\n\")\n",
    "print(\"Hidden Layer 1 Output (ReLU):\")\n",
    "for idx, val in enumerate(H1, 1):\n",
    "    print(f\"  Neuron {idx}: {val:.4f}\")\n",
    "\n",
    "# Step 2: Second Hidden Layer\n",
    "H2 = []\n",
    "for i in range(len(W2)):\n",
    "    z = weighted_sum(H1, W2[i], B2[i])\n",
    "    a = activation_function(z, \"sigmoid\")\n",
    "    H2.append(a)\n",
    "\n",
    "print(\"\\nHidden Layer 2 Output (Sigmoid):\")\n",
    "for idx, val in enumerate(H2, 1):\n",
    "    print(f\"  Neuron {idx}: {val:.4f}\")\n",
    "\n",
    "# Step 3: Output Layer (Sigmoid)\n",
    "Z3 = weighted_sum(H2, W3[0], B3[0])\n",
    "final_output = activation_function(Z3, \"sigmoid\")\n",
    "\n",
    "print(\"\\nFinal Output (Sigmoid):\")\n",
    "print(f\"  Predicted probability (Malignant = 1): {final_output:.4f}\")\n",
    "\n",
    "# Step 4: Predicted Class\n",
    "predicted_class = 1 if final_output >= 0.5 else 0\n",
    "class_name = \"Malignant (1)\" if predicted_class == 1 else \"Benign (0)\"\n",
    "print(f\"\\nPredicted Class: {class_name}\")\n",
    "\n",
    "# Step 5: Loss\n",
    "mse_loss = calculate_loss(final_output, target_output[0], loss_type=\"mse\")\n",
    "ce_loss = calculate_loss(final_output, target_output[0], loss_type=\"cross_entropy\")\n",
    "\n",
    "print(\"\\nLoss:\")\n",
    "print(f\"  MSE: {mse_loss:.4f}\")\n",
    "print(f\"  Cross-Entropy: {ce_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27f52b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 - Hidden Layer 1 (ReLU):\n",
      "  Neuron 1: 3.9300\n",
      "  Neuron 2: 0.1500\n",
      "  Neuron 3: 0.8500\n",
      "\n",
      "Step 2 - Hidden Layer 2 (Sigmoid):\n",
      "  Neuron 1: 0.993782\n",
      "  Neuron 2: 0.991878\n",
      "\n",
      "Step 3 - Output Layer (Softmax probabilities):\n",
      "  Iris-setosa: 0.026507\n",
      "  Iris-versicolor: 0.968651\n",
      "  Iris-virginica: 0.004841\n",
      "\n",
      "Predicted Class: Iris-versicolor\n",
      "\n",
      "Losses:\n",
      "  Cross-Entropy: 3.080656\n",
      "  MSE: 0.351157\n"
     ]
    }
   ],
   "source": [
    "# Problem A: Iris Dataset\n",
    "import math\n",
    "from neural_network_helper import weighted_sum, activation_function\n",
    "\n",
    "# Iris dataset input (Sepal length, Sepal width, Petal length, Petal width)\n",
    "X = [5.1, 3.5, 1.4, 0.2]\n",
    "target_output = [0.7, 0.2, 0.1]  # given target distribution\n",
    "\n",
    "# First Hidden Layer (ReLU)\n",
    "W1 = [\n",
    "    [0.2, 0.1, -0.4, 0.6],   # Neuron 1\n",
    "    [0.5, -0.2, 0.3, -0.1],  # Neuron 2\n",
    "    [-0.3, 0.4, 0.2, 0.5]    # Neuron 3\n",
    "]\n",
    "B1 = [3.0, -2.1, 0.6]\n",
    "\n",
    "# Second Hidden Layer (Sigmoid)\n",
    "W2 = [\n",
    "    [0.3, 0.7, -0.6],  # Neuron 1\n",
    "    [-0.5, 0.2, 0.4]   # Neuron 2\n",
    "]\n",
    "B2 = [4.3, 6.4]\n",
    "\n",
    "# Output Layer (Softmax)\n",
    "W3 = [\n",
    "    [0.5, -0.2],   # Output neuron 1\n",
    "    [-0.3, 0.6],   # Output neuron 2\n",
    "    [0.8, -0.4]    # Output neuron 3\n",
    "]\n",
    "B3 = [-1.5, 2.1, -3.3]\n",
    "\n",
    "# Step 1: First Hidden Layer\n",
    "H1 = []\n",
    "for i in range(len(W1)):\n",
    "    z = weighted_sum(X, W1[i], B1[i])\n",
    "    a = activation_function(z, \"relu\")\n",
    "    H1.append(a)\n",
    "print(\"Step 1 - Hidden Layer 1 (ReLU):\")\n",
    "for idx, val in enumerate(H1, 1):\n",
    "    print(f\"  Neuron {idx}: {val:.4f}\")\n",
    "\n",
    "# Step 2: Second Hidden Layer\n",
    "H2 = []\n",
    "for i in range(len(W2)):\n",
    "    z = weighted_sum(H1, W2[i], B2[i])\n",
    "    a = activation_function(z, \"sigmoid\")\n",
    "    H2.append(a)\n",
    "print(\"\\nStep 2 - Hidden Layer 2 (Sigmoid):\")\n",
    "for idx, val in enumerate(H2, 1):\n",
    "    print(f\"  Neuron {idx}: {val:.6f}\")\n",
    "\n",
    "# Step 3: Output Layer (Softmax)\n",
    "Z3 = []\n",
    "for i in range(len(W3)):\n",
    "    z = weighted_sum(H2, W3[i], B3[i])\n",
    "    Z3.append(z)\n",
    "\n",
    "# stable softmax\n",
    "m = max(Z3)\n",
    "exp_values = [math.exp(z - m) for z in Z3]\n",
    "sum_exp = sum(exp_values)\n",
    "softmax_output = [val / sum_exp for val in exp_values]\n",
    "\n",
    "print(\"\\nStep 3 - Output Layer (Softmax probabilities):\")\n",
    "classes = [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"]\n",
    "for cls, val in zip(classes, softmax_output):\n",
    "    print(f\"  {cls}: {val:.6f}\")\n",
    "\n",
    "# Step 4: Predicted Class\n",
    "predicted_class = classes[softmax_output.index(max(softmax_output))]\n",
    "print(f\"\\nPredicted Class: {predicted_class}\")\n",
    "\n",
    "# Step 5: Losses\n",
    "# Cross-Entropy\n",
    "epsilon = 1e-12\n",
    "cross_entropy_loss = -sum(\n",
    "    y_true * math.log(max(min(y_pred, 1 - epsilon), epsilon))\n",
    "    for y_pred, y_true in zip(softmax_output, target_output)\n",
    ")\n",
    "\n",
    "# MSE\n",
    "mse_loss = sum((y_pred - y_true) ** 2 for y_pred, y_true in zip(softmax_output, target_output)) / len(target_output)\n",
    "\n",
    "print(f\"\\nLosses:\")\n",
    "print(f\"  Cross-Entropy: {cross_entropy_loss:.6f}\")\n",
    "print(f\"  MSE: {mse_loss:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
